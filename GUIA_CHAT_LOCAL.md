# üöÄ Guia Codex-IA: Chat 100% Local e Gr√°tis

Para parar de depender da API do Gemini e n√£o ter mais sustos com a conta, siga estes 3 passos simples para configurar o seu "C√©rebro Local".

## 1. Instalar o Ollama
O Ollama √© a ferramenta que permite rodar IAs poderosas direto no seu Windows.
- **Download:** [Acesse ollama.com](https://ollama.com) e baixe a vers√£o para Windows.
- Instale como qualquer programa normal.

## 2. Baixar o Modelo (Otimizado para o seu PC)
Como sua placa de v√≠deo √© a **Intel HD 620**, selecionamos um modelo super leve e inteligente que vai rodar bem sem travar seu computador.

Abra o seu Terminal (PowerShell ou CMD) e digite:
```powershell
ollama pull llama3.2:3b
```
> [!TIP]
> Este modelo tem "apenas" 2GB, ent√£o o download √© r√°pido e ele n√£o consome muita mem√≥ria RAM.

### Op√ß√£o B: Via GUI (Novo!)
No seu Codex-IA, v√° na aba **"IoT Lab"**:
1. Voc√™ ver√° uma se√ß√£o chamada **"Local Model Manager (Ollama)"**.
2. Clique no bot√£o do modelo desejado (recomendo o **Llama 3.2 3B**).
3. O Codex-IA far√° o download para voc√™ em segundo plano.

## 3. Usar no Codex-IA
No Chat do seu **Codex-IA**, agora voc√™ ver√° um seletor no topo e **√≠cones de status**:
1. **√çcones de Status:** Verifique se o √≠cone de computador (Ollama) est√° **verde**.
2. Clique no seletor (que deve estar em "Brain Router (Auto)").
3. Selecione **"Ollama (Local - 0 Custo)"**.
4. **Pronto!** Suas mensagens agora ser√£o processadas 100% no seu PC.

---

### Por que usar o Local?
- **Totalmente Gr√°tis:** Pode conversar 24h por dia sem gastar 1 centavo.
- **Privacidade M√°xima:** Seus segredos de c√≥digo n√£o saem da sua m√°quina.
- **Offline:** Funciona mesmo se voc√™ estiver sem internet.

> [!IMPORTANT]
> Lembre-se de deixar o √≠cone do Ollama aberto na barra de tarefas (perto do rel√≥gio) para o Codex-IA conseguir se conectar a ele.
